{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1: What is the role of filters and feature maps in Convolutional Neural Network (CNN)?\n",
        "\n",
        "Answer: In a CNN, filters detect important patterns in an image like edges and shapes. When a filter is applied, it creates a feature map, which shows where those patterns appear in the image.\n",
        "\n",
        "## Question 2: Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect the output dimensions of feature maps?\n",
        "\n",
        "Answer: Padding\n",
        "\n",
        "* Padding means adding extra pixels (usually zeros) around the image.\n",
        "\n",
        "    * It helps keep the image size.\n",
        "\n",
        "    * It stops loss of information at the edges.\n",
        "\n",
        "* Stride\n",
        "\n",
        "    * Stride is how many pixels the filter moves each time.\n",
        "\n",
        "    * Small stride - large feature map\n",
        "\n",
        "    * Large stride - small feature map\n",
        "\n",
        "\n",
        "They affect the output dimensions of feature maps as :\n",
        "\n",
        "Padding increases or keeps the size of the feature map by adding extra pixels around the input.\n",
        "\n",
        "Stride reduces the size of the feature map when the filter moves more pixels at a time.\n",
        "\n",
        "## Question 3: Define receptive field in the context of CNNs. Why is it important for deep architectures?\n",
        "\n",
        "Answer: In a CNN, the receptive field is the part of the input image that a neuron looks at to make a decision.\n",
        "\n",
        " It is important because\n",
        "\n",
        "* It helps the network see more area of the image.\n",
        "\n",
        "* In deep CNNs, a larger receptive field helps learn big and complex patterns like objects, not just edges.\n",
        "\n",
        "\n",
        "\n",
        "## Question 4: Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "\n",
        "\n",
        "Answer:\n",
        "* Filter size:\n",
        "   *  Bigger filters have more parameters because they contain more weights.\n",
        "    * Small filters have fewer parameters.\n",
        "\n",
        "* Stride:\n",
        "    * Stride does not change the number of parameters.\n",
        "    * It only changes the size of the feature map.\n",
        "\n",
        "## Question 5: Compare and contrast different CNN-based architectures like LeNet, AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "\n",
        "Answer:\n",
        "* LeNet\n",
        "\n",
        "    * Depth: Shallow (few layers)\n",
        "\n",
        "    * Filter size: Small (mostly 5×5)\n",
        "\n",
        "    * Performance: Good for simple tasks like digit recognition\n",
        "\n",
        "* AlexNet\n",
        "\n",
        "    * Depth: Medium (8 layers)\n",
        "\n",
        "     * Filter size: Large at first (11×11, 5×5)\n",
        "\n",
        "     * Performance: Much better; works well on large image dataset\n",
        "\n",
        "* VGG\n",
        "\n",
        "    * Depth: Deep (16–19 layers)\n",
        "\n",
        "    * Filter size: Very small (only 3×3)\n",
        "\n",
        "    * Performance: High accuracy but slow and needs more memory\n",
        "\n"
      ],
      "metadata": {
        "id": "DftJid0fW-l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "# from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "# Import libraries\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize\n",
        "X_train = X_train.reshape(60000, 28, 28, 1) / 255.0\n",
        "X_test = X_test.reshape(10000, 28, 28, 1) / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Build CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64)\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "TFEuKSAAXty4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4395dfef-78df-4b36-f98c-77ba008643ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 31ms/step - accuracy: 0.8950 - loss: 0.3586\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 29ms/step - accuracy: 0.9819 - loss: 0.0615\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 32ms/step - accuracy: 0.9891 - loss: 0.0371\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 31ms/step - accuracy: 0.9924 - loss: 0.0245\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 30ms/step - accuracy: 0.9946 - loss: 0.0173\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9803 - loss: 0.0631\n",
            "Test Accuracy: 0.9847000241279602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "# CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "\n",
        "# Import libraries\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocessing\n",
        "X_train = X_train / 255.0   # normalize\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# CNN model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64)\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j2d5CHI2Xy-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39110b9b-406e-46c6-e7a0-e71601cd8eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 67ms/step - accuracy: 0.3788 - loss: 1.7197\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 67ms/step - accuracy: 0.5851 - loss: 1.1735\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 68ms/step - accuracy: 0.6445 - loss: 1.0139\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 67ms/step - accuracy: 0.6822 - loss: 0.9196\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 73ms/step - accuracy: 0.7088 - loss: 0.8414\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6925 - loss: 0.9056\n",
            "Test Accuracy: 0.6819999814033508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "# dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load MNIST\n",
        "train_data = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# CNN Model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(1, 32, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 13 * 13, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Loss & Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "for epoch in range(5):\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "correct, total = 0, 0\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"Test Accuracy:\", 100 * correct / total)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9XqKHza8XzAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d9b80c-dac9-43c7-83f6-998f65e9cdbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 56.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.78MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 15.0MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.73MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.2498\n",
            "Epoch 2, Loss: 0.0869\n",
            "Epoch 3, Loss: 0.0628\n",
            "Epoch 4, Loss: 0.0279\n",
            "Epoch 5, Loss: 0.0048\n",
            "Test Accuracy: 98.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Given a custom image dataset stored in a local directory,\n",
        "# write code using Keras  ImageDataGenerator to preprocess and train a CNN model.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "\n",
        "# 1. Create dummy image dataset\n",
        "\n",
        "def create_dummy_image(path, size=(64, 64), color=(0, 0, 0)):\n",
        "    img = Image.new(\"RGB\", size, color)\n",
        "    img.save(path)\n",
        "\n",
        "data_dir = \"dataset\"\n",
        "\n",
        "for subset in [\"train\", \"validation\"]:\n",
        "    for class_name in [\"class1\", \"class2\"]:\n",
        "        folder = os.path.join(data_dir, subset, class_name)\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "        for i in range(5):\n",
        "            img_path = os.path.join(folder, f\"image_{i}.png\")\n",
        "            if class_name == \"class1\":\n",
        "                create_dummy_image(img_path, color=(255, 0, 0))  # red\n",
        "            else:\n",
        "                create_dummy_image(img_path, color=(0, 255, 0))  # green\n",
        "\n",
        "print(\"Dataset created\")\n",
        "\n",
        "# 2. ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    \"dataset/train\",\n",
        "    target_size=(64, 64),\n",
        "    batch_size=4,\n",
        "    class_mode=\"binary\"\n",
        ")\n",
        "\n",
        "val_data = val_datagen.flow_from_directory(\n",
        "    \"dataset/validation\",\n",
        "    target_size=(64, 64),\n",
        "    batch_size=4,\n",
        "    class_mode=\"binary\"\n",
        ")\n",
        "\n",
        "# 3. CNN model\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation=\"relu\", input_shape=(64,64,3)),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation=\"relu\"),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "# 4. Train the model\n",
        "\n",
        "model.fit(\n",
        "    train_data,\n",
        "    epochs=3,\n",
        "    validation_data=val_data\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hgVGxYDsXzDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1367879-c568-4f90-98af-5fe55f608cfb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created\n",
            "Found 10 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n",
            "Epoch 1/3\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 165ms/step - accuracy: 0.5667 - loss: 0.4732 - val_accuracy: 1.0000 - val_loss: 0.0117\n",
            "Epoch 2/3\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 5.0309e-05\n",
            "Epoch 3/3\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 1.8145e-05 - val_accuracy: 1.0000 - val_loss: 6.3984e-07\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7928e0f35af0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Question 10: You are working on a web application for a medical imaging startup. Your task is to build and deploy a CNN model that classifies chest X-ray images into “Normal” and “Pneumonia” categories. Describe your end-to-end approach–from data preparation and model training to deploying the model as a web app using Streamlit\n",
        "\n",
        "Answer : End-to-End approach for Chest X-ray classification\n",
        "1. Data preparation\n",
        "\n",
        "* Collect chest X-ray images labeled Normal and Pneumonia.\n",
        "\n",
        "* Resize images (e.g., 224×224).\n",
        "\n",
        "* Normalize pixel values (0–1).\n",
        "\n",
        "* Split data into train, validation, and test folders.\n",
        "\n",
        "* Use data augmentation (rotation, flip) to avoid overfitting.\n",
        "\n",
        "2. Model training\n",
        "\n",
        "* Use a CNN or pretrained model (like ResNet / VGG).\n",
        "\n",
        "* Add final Dense layer with 2 classes and softmax.\n",
        "\n",
        "* Compile with Adam optimizer and categorical cross-entropy.\n",
        "\n",
        "* Train the model and save it (model.h5).\n",
        "\n",
        "3. Model evaluation\n",
        "\n",
        "* Check accuracy, precision, recall, and confusion matrix.\n",
        "\n",
        "* Make sure false negatives are low (important in medical cases).\n",
        "\n",
        "4. Deployment using Streamlit\n",
        "\n",
        " * Create a Streamlit app with image upload option.\n",
        "\n",
        " * Load the trained model.\n",
        "\n",
        "* Preprocess uploaded X-ray image.\n",
        "\n",
        "* Predict class and show result (Normal / Pneumonia).\n",
        "\n",
        "* Display confidence score.\n",
        "\n",
        "5. Web app flow\n",
        "\n",
        "User uploads X-ray - image is processed - CNN predicts - result shown on screen."
      ],
      "metadata": {
        "id": "7VN427l41Q-u"
      }
    }
  ]
}